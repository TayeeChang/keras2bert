# Keras BERT

Transformer家族模型实现，可加载官方预训练权重来支持下游任务。

目前支持的Transformer模型：   
- [BERT](https://arxiv.org/pdf/1810.04805.pdf&usg=ALkJrhhzxlCL6yTht2BRmH9atgvKFxHsxQ)  
- [Roberta](https://arxiv.org/pdf/1907.11692.pdf%5C)   
- [Albert](https://arxiv.org/pdf/1909.11942.pdf?ref=https://githubhelp.com)  
- [Nezha](https://arxiv.org/pdf/1909.00204.pdf)   
- [Unilm](https://proceedings.neurips.cc/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf)  
- [Electra](https://openreview.net/pdf?id=r1xMH1BtvB)   
- [GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
- [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

后续会陆续添加其他Transformer模型。

继续完善中...

欢迎使用

## 说明

   环境使用  
   - keras >= 2.3.1  
   - tensorflow == 1.13 or 1.14 (建议)
   
## 安装
```shell   
pip install git+https://github.com/TayeeChang/keras2bert.git
```
或者
```shell
python setup.py install
```

## 使用
 
 具体请看example
